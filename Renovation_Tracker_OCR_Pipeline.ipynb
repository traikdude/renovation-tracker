{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üèóÔ∏è Renovation Tracker - Handwritten OCR Pipeline\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/traikdude/renovation-tracker/blob/master/Renovation_Tracker_OCR_Pipeline.ipynb)\n",
    "\n",
    "This notebook provides a complete OCR pipeline for processing handwritten renovation notes, task lists, and financial documents. It extracts data and uploads it to Google Sheets for easy tracking.\n",
    "\n",
    "## üìã What This Notebook Includes:\n",
    "\n",
    "1. **Quick Demo** - Test OCR on sample images\n",
    "2. **Full Pipeline** - Complete processing with all parsers\n",
    "3. **Interactive Mode** - Upload your own images and process them\n",
    "4. **Google Sheets Integration** - Authenticate and upload data\n",
    "5. **Visualizations** - See confidence scores and results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-header"
   },
   "source": [
    "## üîß Section 1: Setup & Installation\n",
    "\n",
    "First, we'll install all required dependencies including Tesseract OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install system dependencies\n",
    "!apt-get update\n",
    "!apt-get install -y tesseract-ocr\n",
    "!apt-get install -y libtesseract-dev\n",
    "\n",
    "# Install Python packages\n",
    "!pip install pytesseract opencv-python-headless pillow gspread google-auth google-api-python-client pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify-installation"
   },
   "outputs": [],
   "source": [
    "# Verify Tesseract installation\n",
    "import pytesseract\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ Tesseract version:\", pytesseract.get_tesseract_version())\n",
    "print(\"‚úÖ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone-repo-header"
   },
   "source": [
    "## üì¶ Section 2: Clone Repository & Import Modules\n",
    "\n",
    "Clone the renovation-tracker repository from GitHub to access all the parsers and utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/traikdude/renovation-tracker.git\n",
    "%cd renovation-tracker\n",
    "\n",
    "print(\"‚úÖ Repository cloned successfully!\")\n",
    "print(\"\\nüìÅ Project files:\")\n",
    "!ls -1 *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-modules"
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "# Import parsers\n",
    "from transaction_parser import TransactionParser\n",
    "from task_parser import TaskParser\n",
    "from layout_parser import LayoutParser\n",
    "from moving_guide_parser import MovingGuideParser\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quick-demo-header"
   },
   "source": [
    "## üöÄ Section 3: Quick Demo - Test OCR\n",
    "\n",
    "Let's test the OCR on sample images included in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quick-demo"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image as IPImage\n",
    "import glob\n",
    "\n",
    "# List available sample images\n",
    "sample_images = glob.glob(\"images/*.jpg\") + glob.glob(\"images/*.jpeg\")\n",
    "print(f\"üì∏ Found {len(sample_images)} sample images\\n\")\n",
    "\n",
    "# Display first sample image\n",
    "if sample_images:\n",
    "    test_image = sample_images[0]\n",
    "    print(f\"Testing with: {os.path.basename(test_image)}\")\n",
    "    display(IPImage(filename=test_image, width=400))\n",
    "    \n",
    "    # Run OCR\n",
    "    img = cv2.imread(test_image)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    text = pytesseract.image_to_string(gray)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìù EXTRACTED TEXT:\")\n",
    "    print(\"=\"*50)\n",
    "    print(text[:500] + \"...\" if len(text) > 500 else text)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No sample images found. You can upload your own in the next section!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interactive-header"
   },
   "source": [
    "## üì§ Section 4: Interactive Mode - Upload Your Own Images\n",
    "\n",
    "Upload your handwritten notes, receipts, or task lists and process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-widget"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from IPython.display import display, Image as IPImage\n",
    "import io\n",
    "\n",
    "print(\"üì§ Upload one or more images to process\")\n",
    "print(\"Supported formats: .jpg, .jpeg, .png\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Create uploads directory\n",
    "!mkdir -p uploads\n",
    "\n",
    "uploaded_files = []\n",
    "for filename in uploaded.keys():\n",
    "    filepath = f\"uploads/{filename}\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(uploaded[filename])\n",
    "    uploaded_files.append(filepath)\n",
    "    print(f\"‚úÖ Saved: {filename}\")\n",
    "\n",
    "print(f\"\\nüì∏ Total uploaded: {len(uploaded_files)} image(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display-uploaded"
   },
   "outputs": [],
   "source": [
    "# Display uploaded images\n",
    "for filepath in uploaded_files:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìÑ {os.path.basename(filepath)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    display(IPImage(filename=filepath, width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "process-header"
   },
   "source": [
    "## üîç Section 5: Process Images with OCR\n",
    "\n",
    "Extract text from uploaded images with confidence scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "process-ocr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_image_with_confidence(image_path):\n",
    "    \"\"\"Process image and return text with confidence scores\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    # Get text and confidence data\n",
    "    data = pytesseract.image_to_data(gray, output_type=pytesseract.Output.DICT)\n",
    "    text = pytesseract.image_to_string(gray)\n",
    "    \n",
    "    # Calculate average confidence\n",
    "    confidences = [int(conf) for conf in data['conf'] if int(conf) > 0]\n",
    "    avg_confidence = sum(confidences) / len(confidences) if confidences else 0\n",
    "    \n",
    "    return text, avg_confidence, data\n",
    "\n",
    "# Process all uploaded images\n",
    "results = []\n",
    "\n",
    "for filepath in uploaded_files:\n",
    "    filename = os.path.basename(filepath)\n",
    "    print(f\"\\nüîÑ Processing: {filename}...\")\n",
    "    \n",
    "    text, confidence, data = process_image_with_confidence(filepath)\n",
    "    \n",
    "    results.append({\n",
    "        'filename': filename,\n",
    "        'text': text,\n",
    "        'confidence': confidence,\n",
    "        'word_count': len(text.split())\n",
    "    })\n",
    "    \n",
    "    print(f\"   ‚úÖ Confidence: {confidence:.1f}%\")\n",
    "    print(f\"   ‚úÖ Words extracted: {len(text.split())}\")\n",
    "    print(f\"\\n   Preview: {text[:200]}...\")\n",
    "\n",
    "# Create results dataframe\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä PROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "display(df_results[['filename', 'confidence', 'word_count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "parsers-header"
   },
   "source": [
    "## üß© Section 6: Intelligent Parsers\n",
    "\n",
    "Use specialized parsers to extract structured data from different document types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "transaction-parser-header"
   },
   "source": [
    "### üí∞ Transaction Parser - Financial Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transaction-parser"
   },
   "outputs": [],
   "source": [
    "# Initialize transaction parser\n",
    "transaction_parser = TransactionParser()\n",
    "\n",
    "print(\"üí∞ TRANSACTION PARSER\")\n",
    "print(\"=\"*60)\n",
    "print(\"Use this for: receipts, expense notes, budget sheets\\n\")\n",
    "\n",
    "# Process financial documents\n",
    "for result in results:\n",
    "    text = result['text']\n",
    "    \n",
    "    # Check if this looks like a financial document\n",
    "    if any(keyword in text.lower() for keyword in ['$', 'price', 'total', 'cost', 'paid', 'expense']):\n",
    "        print(f\"\\nüìÑ Processing: {result['filename']}\")\n",
    "        transactions = transaction_parser.parse(text)\n",
    "        \n",
    "        if transactions:\n",
    "            df_trans = pd.DataFrame(transactions)\n",
    "            print(f\"   ‚úÖ Found {len(transactions)} transaction(s)\")\n",
    "            display(df_trans)\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è No transactions detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "task-parser-header"
   },
   "source": [
    "### ‚úÖ Task Parser - Checklists & To-Do Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "task-parser"
   },
   "outputs": [],
   "source": [
    "# Initialize task parser\n",
    "task_parser = TaskParser()\n",
    "\n",
    "print(\"‚úÖ TASK PARSER\")\n",
    "print(\"=\"*60)\n",
    "print(\"Use this for: checklists, to-do lists, task lists\\n\")\n",
    "\n",
    "# Process task documents\n",
    "for result in results:\n",
    "    text = result['text']\n",
    "    \n",
    "    # Check if this looks like a task list\n",
    "    if any(keyword in text.lower() for keyword in ['task', 'todo', 'checklist', '[ ]', '[x]', 'complete']):\n",
    "        print(f\"\\nüìÑ Processing: {result['filename']}\")\n",
    "        tasks = task_parser.parse(text)\n",
    "        \n",
    "        if tasks:\n",
    "            df_tasks = pd.DataFrame(tasks)\n",
    "            print(f\"   ‚úÖ Found {len(tasks)} task(s)\")\n",
    "            display(df_tasks)\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è No tasks detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "layout-parser-header"
   },
   "source": [
    "### üìê Layout Parser - Property Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "layout-parser"
   },
   "outputs": [],
   "source": [
    "# Initialize layout parser\n",
    "layout_parser = LayoutParser()\n",
    "\n",
    "print(\"üìê LAYOUT PARSER\")\n",
    "print(\"=\"*60)\n",
    "print(\"Use this for: property layouts, room dimensions, floor plans\\n\")\n",
    "\n",
    "# Process layout documents\n",
    "for result in results:\n",
    "    text = result['text']\n",
    "    \n",
    "    # Check if this looks like a layout document\n",
    "    if any(keyword in text.lower() for keyword in ['room', 'ft', 'feet', 'dimension', 'area', 'bedroom', 'kitchen']):\n",
    "        print(f\"\\nüìÑ Processing: {result['filename']}\")\n",
    "        layouts = layout_parser.parse(text)\n",
    "        \n",
    "        if layouts:\n",
    "            df_layouts = pd.DataFrame(layouts)\n",
    "            print(f\"   ‚úÖ Found {len(layouts)} room/area(s)\")\n",
    "            display(df_layouts)\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è No layout information detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moving-parser-header"
   },
   "source": [
    "### üì¶ Moving Guide Parser - Moving Checklists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moving-parser"
   },
   "outputs": [],
   "source": [
    "# Initialize moving guide parser\n",
    "moving_parser = MovingGuideParser()\n",
    "\n",
    "print(\"üì¶ MOVING GUIDE PARSER\")\n",
    "print(\"=\"*60)\n",
    "print(\"Use this for: moving checklists, packing guides, relocation tasks\\n\")\n",
    "\n",
    "# Process moving documents\n",
    "for result in results:\n",
    "    text = result['text']\n",
    "    \n",
    "    # Check if this looks like a moving guide\n",
    "    if any(keyword in text.lower() for keyword in ['moving', 'packing', 'pack', 'box', 'relocation']):\n",
    "        print(f\"\\nüìÑ Processing: {result['filename']}\")\n",
    "        moving_tasks = moving_parser.parse(text)\n",
    "        \n",
    "        if moving_tasks:\n",
    "            df_moving = pd.DataFrame(moving_tasks)\n",
    "            print(f\"   ‚úÖ Found {len(moving_tasks)} moving task(s)\")\n",
    "            display(df_moving)\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è No moving tasks detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization-header"
   },
   "source": [
    "## üìä Section 7: Visualizations\n",
    "\n",
    "Visualize OCR confidence scores and processing statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualizations"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if results:\n",
    "    # Confidence scores bar chart\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Confidence Scores\n",
    "    filenames = [r['filename'] for r in results]\n",
    "    confidences = [r['confidence'] for r in results]\n",
    "    \n",
    "    axes[0].bar(range(len(filenames)), confidences, color='steelblue')\n",
    "    axes[0].set_xticks(range(len(filenames)))\n",
    "    axes[0].set_xticklabels([f[:15] + '...' if len(f) > 15 else f for f in filenames], rotation=45, ha='right')\n",
    "    axes[0].set_ylabel('Confidence (%)')\n",
    "    axes[0].set_title('OCR Confidence Scores by Image')\n",
    "    axes[0].axhline(y=70, color='r', linestyle='--', label='Good threshold (70%)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Word Count\n",
    "    word_counts = [r['word_count'] for r in results]\n",
    "    \n",
    "    axes[1].bar(range(len(filenames)), word_counts, color='coral')\n",
    "    axes[1].set_xticks(range(len(filenames)))\n",
    "    axes[1].set_xticklabels([f[:15] + '...' if len(f) > 15 else f for f in filenames], rotation=45, ha='right')\n",
    "    axes[1].set_ylabel('Word Count')\n",
    "    axes[1].set_title('Words Extracted per Image')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìà SUMMARY STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Average Confidence: {sum(confidences)/len(confidences):.1f}%\")\n",
    "    print(f\"Total Words Extracted: {sum(word_counts)}\")\n",
    "    print(f\"Images Processed: {len(results)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results to visualize. Upload and process images first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "google-sheets-header"
   },
   "source": [
    "## üìä Section 8: Google Sheets Integration\n",
    "\n",
    "Upload extracted data to Google Sheets for tracking and management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "google-auth"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "from google.auth import default\n",
    "import gspread\n",
    "\n",
    "print(\"üîê Google Sheets Authentication\\n\")\n",
    "print(\"This will allow the notebook to access your Google Sheets.\")\n",
    "print(\"Click the link below and authorize access.\\n\")\n",
    "\n",
    "# Authenticate with Google\n",
    "auth.authenticate_user()\n",
    "creds, _ = default()\n",
    "\n",
    "# Initialize gspread client\n",
    "gc = gspread.authorize(creds)\n",
    "\n",
    "print(\"‚úÖ Authentication successful!\")\n",
    "print(\"\\nYou can now upload data to Google Sheets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-to-sheets"
   },
   "outputs": [],
   "source": [
    "# Upload data to Google Sheets\n",
    "from datetime import datetime\n",
    "\n",
    "def upload_to_google_sheets(spreadsheet_id, worksheet_name, data_df):\n",
    "    \"\"\"Upload DataFrame to Google Sheets\"\"\"\n",
    "    try:\n",
    "        # Open spreadsheet\n",
    "        spreadsheet = gc.open_by_key(spreadsheet_id)\n",
    "        \n",
    "        # Get or create worksheet\n",
    "        try:\n",
    "            worksheet = spreadsheet.worksheet(worksheet_name)\n",
    "        except:\n",
    "            worksheet = spreadsheet.add_worksheet(title=worksheet_name, rows=100, cols=20)\n",
    "        \n",
    "        # Add timestamp column\n",
    "        data_df['Uploaded At'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Convert to list format for gspread\n",
    "        values = [data_df.columns.tolist()] + data_df.values.tolist()\n",
    "        \n",
    "        # Clear existing data and upload new data\n",
    "        worksheet.clear()\n",
    "        worksheet.update('A1', values)\n",
    "        \n",
    "        print(f\"‚úÖ Uploaded {len(data_df)} rows to '{worksheet_name}'\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error uploading to Google Sheets: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "print(\"üì§ UPLOAD TO GOOGLE SHEETS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTo upload data, you need:\")\n",
    "print(\"1. Your Google Spreadsheet ID (from the URL)\")\n",
    "print(\"2. The worksheet name to upload to\\n\")\n",
    "\n",
    "# Uncomment and fill in your details to upload:\n",
    "# SPREADSHEET_ID = \"your_spreadsheet_id_here\"\n",
    "# WORKSHEET_NAME = \"OCR Results\"\n",
    "# upload_to_google_sheets(SPREADSHEET_ID, WORKSHEET_NAME, df_results)\n",
    "\n",
    "print(\"üí° Tip: Uncomment the code above and add your spreadsheet ID to upload results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "google-drive-header"
   },
   "source": [
    "## üíæ Section 9: Google Drive Integration (Optional)\n",
    "\n",
    "Mount Google Drive to access images stored in your Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "print(\"üìÅ Mounting Google Drive...\\n\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n‚úÖ Google Drive mounted at /content/drive/\")\n",
    "print(\"\\nYou can now access files from your Google Drive!\")\n",
    "print(\"\\nExample: Process images from Drive folder:\")\n",
    "print(\"   drive_folder = '/content/drive/MyDrive/RenovationImages'\")\n",
    "print(\"   images = glob.glob(f'{drive_folder}/*.jpg')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export-header"
   },
   "source": [
    "## üíæ Section 10: Export Results\n",
    "\n",
    "Save extracted data locally as Excel or CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export-results"
   },
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "# Export results to Excel\n",
    "if results:\n",
    "    output_file = 'renovation_tracker_results.xlsx'\n",
    "    \n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        # Save OCR results summary\n",
    "        df_results.to_excel(writer, sheet_name='OCR Summary', index=False)\n",
    "        \n",
    "        # Save extracted text for each image\n",
    "        for idx, result in enumerate(results):\n",
    "            sheet_name = f\"Text_{idx+1}\"\n",
    "            df_text = pd.DataFrame({'Text': [result['text']]})\n",
    "            df_text.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Results exported to: {output_file}\")\n",
    "    print(f\"   - OCR Summary sheet\")\n",
    "    print(f\"   - {len(results)} text extraction sheets\\n\")\n",
    "    \n",
    "    # Download the file\n",
    "    files.download(output_file)\n",
    "    print(\"üì• Download started!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results to export. Process images first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "full-pipeline-header"
   },
   "source": [
    "## üîÑ Section 11: Full Automated Pipeline\n",
    "\n",
    "Run the complete pipeline: upload ‚Üí OCR ‚Üí parse ‚Üí export ‚Üí upload to Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "full-pipeline"
   },
   "outputs": [],
   "source": [
    "def run_full_pipeline(image_paths, spreadsheet_id=None):\n",
    "    \"\"\"\n",
    "    Complete OCR pipeline:\n",
    "    1. Process images with OCR\n",
    "    2. Run all parsers\n",
    "    3. Generate summary report\n",
    "    4. Export to Excel\n",
    "    5. Upload to Google Sheets (optional)\n",
    "    \"\"\"\n",
    "    print(\"üöÄ FULL PIPELINE STARTED\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    all_results = {\n",
    "        'ocr_summary': [],\n",
    "        'transactions': [],\n",
    "        'tasks': [],\n",
    "        'layouts': [],\n",
    "        'moving_tasks': []\n",
    "    }\n",
    "    \n",
    "    # Step 1: OCR Processing\n",
    "    print(\"\\nüì∏ Step 1: Processing images with OCR...\")\n",
    "    for image_path in image_paths:\n",
    "        filename = os.path.basename(image_path)\n",
    "        print(f\"   - {filename}\")\n",
    "        \n",
    "        text, confidence, _ = process_image_with_confidence(image_path)\n",
    "        \n",
    "        all_results['ocr_summary'].append({\n",
    "            'filename': filename,\n",
    "            'confidence': confidence,\n",
    "            'word_count': len(text.split()),\n",
    "            'text_preview': text[:100]\n",
    "        })\n",
    "        \n",
    "        # Step 2: Run parsers\n",
    "        print(f\"   üîç Running intelligent parsers...\")\n",
    "        \n",
    "        # Transaction parser\n",
    "        transactions = transaction_parser.parse(text)\n",
    "        if transactions:\n",
    "            for t in transactions:\n",
    "                t['source_file'] = filename\n",
    "            all_results['transactions'].extend(transactions)\n",
    "            print(f\"      üí∞ {len(transactions)} transaction(s)\")\n",
    "        \n",
    "        # Task parser\n",
    "        tasks = task_parser.parse(text)\n",
    "        if tasks:\n",
    "            for t in tasks:\n",
    "                t['source_file'] = filename\n",
    "            all_results['tasks'].extend(tasks)\n",
    "            print(f\"      ‚úÖ {len(tasks)} task(s)\")\n",
    "        \n",
    "        # Layout parser\n",
    "        layouts = layout_parser.parse(text)\n",
    "        if layouts:\n",
    "            for l in layouts:\n",
    "                l['source_file'] = filename\n",
    "            all_results['layouts'].extend(layouts)\n",
    "            print(f\"      üìê {len(layouts)} layout(s)\")\n",
    "        \n",
    "        # Moving parser\n",
    "        moving = moving_parser.parse(text)\n",
    "        if moving:\n",
    "            for m in moving:\n",
    "                m['source_file'] = filename\n",
    "            all_results['moving_tasks'].extend(moving)\n",
    "            print(f\"      üì¶ {len(moving)} moving task(s)\")\n",
    "    \n",
    "    # Step 3: Generate summary\n",
    "    print(\"\\nüìä Step 2: Generating summary report...\")\n",
    "    print(f\"   - Images processed: {len(image_paths)}\")\n",
    "    print(f\"   - Transactions found: {len(all_results['transactions'])}\")\n",
    "    print(f\"   - Tasks found: {len(all_results['tasks'])}\")\n",
    "    print(f\"   - Layouts found: {len(all_results['layouts'])}\")\n",
    "    print(f\"   - Moving tasks found: {len(all_results['moving_tasks'])}\")\n",
    "    \n",
    "    # Step 4: Export to Excel\n",
    "    print(\"\\nüíæ Step 3: Exporting to Excel...\")\n",
    "    output_file = f'renovation_tracker_full_pipeline_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.xlsx'\n",
    "    \n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        pd.DataFrame(all_results['ocr_summary']).to_excel(writer, sheet_name='OCR Summary', index=False)\n",
    "        \n",
    "        if all_results['transactions']:\n",
    "            pd.DataFrame(all_results['transactions']).to_excel(writer, sheet_name='Transactions', index=False)\n",
    "        \n",
    "        if all_results['tasks']:\n",
    "            pd.DataFrame(all_results['tasks']).to_excel(writer, sheet_name='Tasks', index=False)\n",
    "        \n",
    "        if all_results['layouts']:\n",
    "            pd.DataFrame(all_results['layouts']).to_excel(writer, sheet_name='Layouts', index=False)\n",
    "        \n",
    "        if all_results['moving_tasks']:\n",
    "            pd.DataFrame(all_results['moving_tasks']).to_excel(writer, sheet_name='Moving Tasks', index=False)\n",
    "    \n",
    "    print(f\"   ‚úÖ Saved: {output_file}\")\n",
    "    \n",
    "    # Step 5: Upload to Google Sheets (if spreadsheet_id provided)\n",
    "    if spreadsheet_id:\n",
    "        print(\"\\nüì§ Step 4: Uploading to Google Sheets...\")\n",
    "        \n",
    "        if all_results['transactions']:\n",
    "            upload_to_google_sheets(spreadsheet_id, 'Transactions', pd.DataFrame(all_results['transactions']))\n",
    "        \n",
    "        if all_results['tasks']:\n",
    "            upload_to_google_sheets(spreadsheet_id, 'Tasks', pd.DataFrame(all_results['tasks']))\n",
    "        \n",
    "        if all_results['layouts']:\n",
    "            upload_to_google_sheets(spreadsheet_id, 'Layouts', pd.DataFrame(all_results['layouts']))\n",
    "        \n",
    "        if all_results['moving_tasks']:\n",
    "            upload_to_google_sheets(spreadsheet_id, 'Moving Tasks', pd.DataFrame(all_results['moving_tasks']))\n",
    "    \n",
    "    print(\"\\n‚úÖ PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Download results\n",
    "    files.download(output_file)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Run the full pipeline on uploaded images\n",
    "if uploaded_files:\n",
    "    # Uncomment to run with Google Sheets upload:\n",
    "    # SPREADSHEET_ID = \"your_spreadsheet_id_here\"\n",
    "    # pipeline_results = run_full_pipeline(uploaded_files, spreadsheet_id=SPREADSHEET_ID)\n",
    "    \n",
    "    # Run without Google Sheets upload:\n",
    "    pipeline_results = run_full_pipeline(uploaded_files)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No images uploaded. Please upload images in Section 4 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tips-header"
   },
   "source": [
    "## üí° Tips & Best Practices\n",
    "\n",
    "### For Best OCR Results:\n",
    "- ‚úÖ Use high-resolution images (300 DPI or higher)\n",
    "- ‚úÖ Ensure good lighting and contrast\n",
    "- ‚úÖ Keep text horizontal (rotate images if needed)\n",
    "- ‚úÖ Avoid blurry or skewed images\n",
    "\n",
    "### Parser Selection:\n",
    "- **Transaction Parser**: Receipts, invoices, expense notes\n",
    "- **Task Parser**: Checklists, to-do lists, project tasks\n",
    "- **Layout Parser**: Room dimensions, property layouts\n",
    "- **Moving Parser**: Moving guides, packing checklists\n",
    "\n",
    "### Google Sheets Setup:\n",
    "1. Create a new Google Sheet or use existing one\n",
    "2. Copy the spreadsheet ID from the URL\n",
    "3. Run the authentication cell in Section 8\n",
    "4. Use the upload functions to push data\n",
    "\n",
    "---\n",
    "\n",
    "## üêõ Troubleshooting\n",
    "\n",
    "**Low Confidence Scores (<70%)**\n",
    "- Improve image quality\n",
    "- Try preprocessing (rotate, enhance contrast)\n",
    "- Check if handwriting is legible\n",
    "\n",
    "**No Data Parsed**\n",
    "- Verify the text contains relevant keywords\n",
    "- Check parser is appropriate for document type\n",
    "- Review extracted text for accuracy\n",
    "\n",
    "**Google Sheets Upload Fails**\n",
    "- Verify authentication completed successfully\n",
    "- Check spreadsheet ID is correct\n",
    "- Ensure you have edit access to the sheet\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- **GitHub Repository**: https://github.com/traikdude/renovation-tracker\n",
    "- **Tesseract Documentation**: https://github.com/tesseract-ocr/tesseract\n",
    "- **gspread Documentation**: https://docs.gspread.org/\n",
    "\n",
    "---\n",
    "\n",
    "**Built with Python ‚Ä¢ Tesseract OCR ‚Ä¢ Google Colab**\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Renovation Tracker OCR Pipeline",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
